<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" href="index.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&display=swap" rel="stylesheet">
    <link href='https://fonts.googleapis.com/css2?family=Lora&display=swap'rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="navbar.js"> </script>
  </head>
    
  <body>
    <nav>
      <ul class="left-ul">
        <li> <a> Zhengguang Wang </a> </li>
      </ul>
      <ul class="right-ul">
        <li><a href="index.html" onclick="update(this)" id="index">Home</a></li>
        <li><a href="research.html" onclick="update(this)" id="research">Research</a></li>
        <li><a href="gadgets.html" onclick="update(this)" id="gadgets">Gadgets and Misc.</a></li>
      </ul>
    </nav>   
    <div class="research"> 
      <h1 class="research-group"> Information and Language Processing Lab</h2>
        <p class="Research-Text"> This lab working on natural language processing (NLP) is led by Professor Yangfeng Ji, where I am an current undergraduate student researcher since June, 2023. 
          Our team convene each week, during which we discuss a NLP research paper, studying the authors' technical skills and paper-writing skills. We also discussed the implications and reasoning abilities of 
          Large Language Models (LLMs). We also focused on Parameter Efficient Fine Tuning methods such as Low-Rank Approximation (LoRA). 
          In particular, I am involved in our group's collective effort to create high-level LLM-finetuning wrapper/codebase built on Huggingface and Pytorch. 
          Our codebase covers the <strong>entire pipeline of LLM fine-tuning </strong>from Preparation, Fine-tuning Methods (LoRA and Prefix Tuning), Open-source LLMs (Llama-2 and Falcon),and Datasets. I have already published 
          a <a class="Research-Text-link" href="https://medium.com/@zw4re/to-a5b4ec215658" target="_blank"> blog</a> on Preparation on Medium, which provides detailed step-by-step guide to help setup the environment and use the HPC for fine-tuning.

        <h2 class="research-topic"> My research topic in ILP lab: Consistency of Political Leaning of Large Language Models</h2>
        <p class="Research-Text"> Despite LLMs' remarkable writing abilities and their seeming 
            omniscience, their consistency and as a result, trustworthiness, are in doubt. We can't 
            reasonably claim LLMs possess an "opinion" and "ability to think" before we have systematically 
            evaluated their consistency of responses across prompts with regard to same essence. 
        </p>
        <p class="Research-Text"> Under the mentorship of Professor Yangfeng Ji, I am working on evaluating how consistent LLMs' political leaning is. 
          I utilized headlines from news articles and tested LLMs using various prompt framings, using different pronouns while maintaining the core essence.
          The headlines were sourced from Allsides.com, the DailyBeast, Breitbart, and were scraped using Selenium and Beautifulsoup. My evaluation involved querying the OpenAI API with both GPT-3.5 and GPT-4.
        </p>
        <h2 class="research-topic"> My research topic in ILP lab: Reliable Text Summarization</h2>
        <p class="Research-Text"> Our group has came to an realization that <strong> "hallucination"</strong> and <strong>"creativity" </strong> are actually two sides of one coin for LLMs. 
          While they excel at creative writing, their document summarizations are often unreliable. On the other hand, Statistical NLP methods are more reliable yet generate
          less fluent sentences. 
        </p>
        <p class="Research-Text"> Under the mentorship of Professor Yangfeng Ji, I am working on engineering reliable Text Summarizations, trying 
          to combine the text fluency generated by LLMs and reliability of Statistical NLP methods such as POS tagging. 
        </p>




      <h1 class="research-group"> UVA-MLSys</h1>
      <p class="Research-Text"> The UVA Machine Learning & System group is actively engaged in AI research, particularly in the domain of AI4Science. 
        I am proud to have been a part of the <a href="https://docs.google.com/presentation/d/1UVwyaaKuqKX30_MOGXs8VYADVIW86rAb/edit#slide=id.p15" target="_blank" class="Research-Text-link"> Global Pervasive Computational Epidemiology (GPCE) project, which focused on interpreting county-level COVID-19 infections using deep learning for time series </a>. 
        Our group won the <a href="https://conferences.computer.org/icdh/2023/student_awards.html" target="_blank" class="Research-Text-link"> 3rd prize at the IEEE ICDH 2023 conference </a>. 
        In this project, I had the privilege of leading a team that developed an <a href="https://zhengguangw.github.io/GPCE-COVID-Website/" class="Research-Text-link">interactive project website</a> to enhance our research's accessibility and impact.
        Building on the success of the GPCE project, our group run more experiments on Sensitivity Analysis methods on County-Level COVID Data. As a coauthor, our group's paper <a href="Age_Sensitivity___AAAI_2024_Workshop.pdf" target="_blank" class="Research-Text-link"><em style="color:#E57200"> 
          Interpreting Time Series Transformer Models and Sensitivity Analysis of Population Age Groups to COVID-19 Infections</em> </a> is accepted to the AAAI'24 Workshop AI4TS:AI For Time Series Analysis.
      </p>
      <h2 class="research-topic"> My research topic in MLSys: Consistency and Performance of Sensitivity Analysis Methods</h2>
      <p class="Research-Text"> State-of-the-art transformer models have remarkable performance dealing with sequential data, surpassing traditional statistical ARIMA models
        at the cost of low interpretability. Thus, various Sensitivity Analysis methods, which can assess how changes in input variables or parameters affect the model's output, have been proposed. 
        I am specifically interested in the performance and consistency of these methods like Morris Method, Feature Ablation, i.e, Would they yield same conclusions, and 
        under what circumstance one would outperform the other? If the methods we used to interprete black-box models are unreliabl and unconsistent themselves, we need to be very cautious in real-life application.
        My research statement on this topic <a href="InterpretationTS___AAAI24UC.pdf" target="_blank" class="Research-Text-link"><em style="color:#E57200"> Validation, Robustness, and Accuracy of Perturbation-based Sensitivity Analysis
          Methods for Time-Series Deep Learning Models</em> </a> is accepted to the AAAI'24 Undergrduate Consortium. 
      </p>
    </div>
  </body>
</html>