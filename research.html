<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" href="index.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&display=swap" rel="stylesheet">
    <link href='https://fonts.googleapis.com/css2?family=Lora&display=swap'rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="navbar.js"> </script>
  </head>
    
  <body>
    <nav>
      <ul class="left-ul">
        <li> <a> Zhengguang Wang </a> </li>
      </ul>
      <ul class="right-ul">
        <li><a href="index.html" onclick="update(this)" id="index">Home</a></li>
        <li><a href="research.html" onclick="update(this)" id="research">Research</a></li>
        <li><a href="gadgets.html" onclick="update(this)" id="gadgets">Gadgets and Misc.</a></li>
      </ul>
    </nav>   
    <div class="research"> 
      <h1 class="research-group"> Information and Language Processing Lab</h2>
        <p class="Research-Text"> This lab working on natural language processing (NLP) is led by Professor Yangfeng Ji, where I am an current undergraduate student researcher since June, 2023. 
          Our team convene each week, during which we discuss a NLP paper, studying the authors' technical skills and paper-writing skills. We also discussed the implications and reasoning abilities of 
          Large Language Models (LLMs).
        </p>
        <h2 class="research-topic"> My research topic in ILP lab: Consistency of Political Leaning of Large Language Models</h2>
        <p class="Research-Text"> Despite LLMs' remarkable writing abilities and their seeming 
            omniscience, their consistency and as a result, trustworthiness, are in doubt. We can't 
            reasonably claim LLMs possess an "opinion" and "ability to think" before we have systematically 
            evaluated their consistency of responses across prompts with regard to same essence. 
        </p>
        <p class="Research-Text"> Under the mentorship of Professor Yangfeng Ji, I am working on evaluating how consistent LLMs' political leaning is. 
          I utilized headlines from news articles and tested LLMs using various prompt framings, using different pronouns while maintaining the core essence.
          The headlines were sourced from Allsides.com, the DailyBeast, Breitbart, and were scraped using Selenium and Beautifulsoup. My evaluation involved querying the OpenAI API with both GPT-3.5 and GPT-4.
          I am working towards a paper submission for <a href="https://2024.naacl.org/calls/papers/">NAACL</a> on December.
          </p>
      <h1 class="research-group"> UVA-MLSys</h1>
      <p class="Research-Text"> The UVA Machine Learning & System group is actively engaged in AI research, particularly in the domain of AI4Science. 
        I am proud to have been a part of the Global Pervasive Computational Epidemiology (GPCE) project, which focused on interpreting county-level COVID-19 infections using deep learning for time series. Our group won the 3rd prize at the IEEE ICDH 2023 conference. 
        In this project, I had the privilege of leading a dedicated team that developed an interactive project website to enhance our research's accessibility and impact.
        Building on the success of the GPCE project, our group has submitted a poster <em>"Time Series Sensitivity Analysis of Population Age Groups in Multi-Horizon COVID-19 Infection Forecasting" </em> to the AAAI'24 poster program. 
        I am currently involved in running experiments to evaluate various Sensitivity Analysis methods such as feature ablation and the Morris method on different time-series Deep Learning models. 
        These efforts are part of our ongoing research as we aim to prepare a paper submission to the AAAI'24 workshop.
      </p>
      <h2 class="research-topic"> My research topic in MLSys: Consistency and Performance of Sensitivity Analysis Methods</h2>
      <p class="Research-Text"> State-of-the-art transformer models have remarkable performance dealing with sequential data, surpassing traditional statistical ARIMA models
        at the cost of low interpretability. Thus, various Sensitivity Analysis methods, which can assess how changes in input variables or parameters affect the model's output, have been proposed. 
        I am specifically interested in the performance and consistency of these methods like Morris Method, Feature Ablation, i.e, Would they yield same conclusions, and 
        under what circumstance one would outperform the other? If the methods we used to interprete black-box models are unreliabl and unconsistent themselves, we need to be very cautious in real-life application.
        I have submitted a research statement <em>Consistencies and Performances of Perturbation-based Sensitivity Analysis Methods</em> to the AAAI'24 Undergrduate Consortium, and I am 
        now building test models and Sensitivity Analysis methods.
      </p>
    </div>
  </body>
</html>