<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" href="index.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&display=swap" rel="stylesheet">
    <link href='https://fonts.googleapis.com/css2?family=Lora&display=swap'rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="navbar.js"> </script>
  </head>
    
  <body>
    <nav>
      <ul class="left-ul">
        <li> <a> Zhengguang Wang </a> </li>
      </ul>
      <ul class="right-ul">
        <li><a href="index.html" onclick="update(this)" id="index">Home</a></li>
        <li><a href="research.html" onclick="update(this)" id="research">Research</a></li>
        <li><a href="gadgets.html" onclick="update(this)" id="gadgets">Gadgets and Misc.</a></li>
      </ul>
    </nav>   
    <div class="research"> 
      <h1 class="research-group"> Information and Language Processing Lab</h2>
        <p class="Research-Text"> This lab working on natural language processing (NLP) is led by Professor Yangfeng Ji, where I am an current undergraduate student researcher since June, 2023. 
          Our team convene each week, during which we discuss a NLP paper, studying the authors' technical skills and paper-writing skills. We also discussed the implications and reasoning abilities of 
          Large Language Models (LLMs).
        </p>
        <h2 class="research-topic"> My research topic in ILP lab: Consistency of Political Leaning of Large Language Models</h2>
        <p class="Research-Text"> Despite LLMs' remarkable writing abilities and their seeming 
            omniscience, their consistency and as a result, trustworthiness, are in doubt. We can't 
            reasonably claim LLMs possess an "opinion" and "ability to think" before we have systematically 
            evaluated their consistency of responses across prompts with regard to same essence. 
        </p>
        <p class="Research-Text"> Under the mentorship of Professor Yangfeng Ji, I am working on evaluating how consistent LLMs' political leaning is. 
          I utilized headlines from news articles and tested LLMs using various prompt framings, using different pronouns while maintaining the core essence.
          The headlines were sourced from Allsides.com, the DailyBeast, Breitbart, and were scraped using Selenium and Beautifulsoup. My evaluation involved querying the OpenAI API with both GPT-3.5 and GPT-4.
          I am working towards a paper submission for <a href="https://2024.naacl.org/calls/papers/">NAACL</a> on December.
          </p>
      <h1 class="research-group"> UVA-MLSys</h1>
      <p class="Research-Text"> The UVA Machine Learning & System group, led by Professor Judy Fox, is working on AI4Science. 
        I have participated in the project <em>Global Pervasive Computational Epidemiology (GPCE) Interpreting County-level Covid Infections using Deep Learning for Time Series</em>, in which I helped our 
        team win 3rd prize at IEEE ICDH 2023 by leading a team to build an interactive project website. Extending this work, our group has submitted a poster <em> Time Series Sensitivity Analysis of Population Age Groups in Multi-Horizon COVID-19 Infection Forecasting</em>
        to the AAAI'24 poster program. Based on the poster, I am actively running experiments evaluating Sensitivity Analysis methods, such as feature ablation and Morris method. Our group is aiming for a paper submission to the AAAI'24 workshop. 
      </p>
      <h2 class="research-topic"> My research topic in MLSys: Consistency and Performance of Sensitivity Analysis Methods</h2>
      <p class="Research-Text"> Despite LLMs' remarkable writing abilities and their seeming 
          omniscience, their consistency and as a result, trustworthiness, are in doubt. We can't 
          reasonably claim LLMs possess an "opinion" and "ability to think" before we have systematically 
          evaluated their consistency of responses across prompts with regard to same essence. 
      </p>
      <p class="Research-Text"> Under the mentorship of Professor Yangfeng Ji, I am working on evaluating how consistent LLMs' political leaning is. 
        I utilized headlines from news articles and tested LLMs using various prompt framings, using different pronouns while maintaining the core essence.
        The headlines were sourced from Allsides.com, the DailyBeast, Breitbart, and were scraped using Selenium and Beautifulsoup. My evaluation involved querying the OpenAI API with both GPT-3.5 and GPT-4.
        I am working towards a paper submission for <a href="https://2024.naacl.org/calls/papers/">NAACL</a> on December.
        </p>
    </div>
  </body>
</html>